#!/bin/bash
# Quick runner script for the OULY scheduler comparison benchmarks
# This script demonstrates the JSON output generation for performance tracking

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
BUILD_DIR="$PROJECT_ROOT/build"
BENCHMARK_EXECUTABLE="$BUILD_DIR/unit_tests/ouly-scheduler-comparison-bench"

# Set up environment
export LD_LIBRARY_PATH="$BUILD_DIR/gnu_15.1_cxx20_64_release:$LD_LIBRARY_PATH"

# Check if benchmark executable exists
if [ ! -f "$BENCHMARK_EXECUTABLE" ]; then
    echo "âŒ Benchmark executable not found: $BENCHMARK_EXECUTABLE"
    echo "ðŸ“‹ Please build the project first with:"
    echo "   cmake --build build --target ouly-scheduler-comparison-bench"
    exit 1
fi

echo "ðŸš€ Running OULY Scheduler Comparison Benchmarks"
echo "================================================"
echo "ðŸ“ Working directory: $(pwd)"
echo "ðŸ”§ Executable: $BENCHMARK_EXECUTABLE"
echo ""

# Run the benchmark
"$BENCHMARK_EXECUTABLE" "$@"

echo ""
echo "ðŸ“Š Benchmark Results:"
echo "===================="

# Show generated JSON files (for performance tracking integration)
if ls scheduler_comparison_*.json 1> /dev/null 2>&1; then
    echo "âœ… JSON files generated for performance tracking:"
    for json_file in scheduler_comparison_*.json; do
        echo "   ðŸ“„ $json_file"
        
        # Show a preview of the JSON structure
        if command -v jq &> /dev/null; then
            echo "      ðŸ“ˆ Preview (first benchmark result):"
            jq -r '(.context // {}) | "         Platform: \(.platform // "unknown")", "         CPU: \(.cpu // "unknown")", "         Date: \(.date // "unknown")"' "$json_file" 2>/dev/null || true
            jq -r '.results[0] | "         First Result: \(.name) - \(.median * 1e9 | round) ns/op"' "$json_file" 2>/dev/null || true
        fi
        echo ""
    done
else
    echo "âš ï¸  No JSON files found. Check if benchmark completed successfully."
fi

# Show generated text files
if ls scheduler_comparison_*.txt 1> /dev/null 2>&1; then
    echo "ðŸ“‹ Human-readable results:"
    for txt_file in scheduler_comparison_*.txt; do
        echo "   ðŸ“„ $txt_file"
    done
    echo ""
fi

echo "ðŸŽ¯ Integration Notes:"
echo "===================="
echo "â€¢ JSON files are compatible with the performance-tracking branch workflow"
echo "â€¢ Files follow nanobench JSON format with full statistical data"
echo "â€¢ Benchmark names include scheduler version (V1, V2, TBB) for comparison"
echo "â€¢ Results include both timing and instruction count metrics"
echo ""
echo "ðŸ“š For CI integration, these files can be processed by:"
echo "â€¢ scripts/analyze_performance.py (trend analysis)"
echo "â€¢ scripts/visualize_performance.py (chart generation)"
echo "â€¢ GitHub Actions performance workflow (automatic tracking)"
