name: CI

on:
  push:
    branches: [ "main", "develop" ]
  pull_request:
    branches: [ "main", "develop" ]

env:
  # Customize the CMake build type here (Release, Debug, RelWithDebInfo, etc.)
  BUILD_TYPE: Debug

jobs:
  build-and-test:
    name: ${{ matrix.config.name }}
    runs-on: ${{ matrix.config.os }}
    strategy:
      fail-fast: false
      matrix:
        config:
          - {
              name: "Windows MSVC",
              os: windows-latest,
              generator: "Ninja",
              cc: "cl",
              cxx: "cl"
            }
          - {
              name: "Ubuntu GCC",
              os: ubuntu-latest,
              generator: "Ninja",
              cc: "gcc-14",
              cxx: "g++-14"
            }
          - {
              name: "Ubuntu Clang",
              os: ubuntu-latest,
              generator: "Ninja", 
              cc: "clang-18",
              cxx: "clang++-18"
            }
          - {
              name: "macOS Clang",
              os: macos-latest,
              generator: "Ninja",
              cc: "clang",
              cxx: "clang++",
              min_clang_version: "17"
            }

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup CMake and Ninja
      uses: lukka/get-cmake@latest

    - name: Install Linux dependencies
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        if [[ "${{ matrix.config.cxx }}" == "g++-14" ]]; then
          sudo apt-get install -y gcc-14 g++-14
        elif [[ "${{ matrix.config.cxx }}" == "clang++-18" ]]; then
          wget https://apt.llvm.org/llvm.sh
          chmod +x llvm.sh
          sudo ./llvm.sh 18
          sudo apt-get install -y clang-18 clang++-18
        fi

    - name: Check macOS Clang version
      if: runner.os == 'macOS'
      id: clang_check
      run: |
        # Check current Clang version
        clang_version=$(clang --version | head -n1 | grep -o '[0-9]\+\.[0-9]\+' | head -n1 | cut -d. -f1)
        echo "Current Clang version: $clang_version"
        echo "clang_version=$clang_version" >> $GITHUB_OUTPUT
        
        if [[ "${{ matrix.config.min_clang_version }}" != "" ]] && [[ $clang_version -lt ${{ matrix.config.min_clang_version }} ]]; then
          echo "Clang version $clang_version is less than required ${{ matrix.config.min_clang_version }}"
          echo "Skipping this build as it's not compatible with older Clang versions"
          echo "skip_build=true" >> $GITHUB_OUTPUT
        else
          echo "skip_build=false" >> $GITHUB_OUTPUT
        fi

    - name: Skip build notification
      if: runner.os == 'macOS' && steps.clang_check.outputs.skip_build == 'true'
      run: |
        echo "::notice::Skipping macOS build due to incompatible Clang version (${{ steps.clang_check.outputs.clang_version }} < ${{ matrix.config.min_clang_version }})"

    - name: Setup MSVC
      if: runner.os == 'Windows'
      uses: ilammy/msvc-dev-cmd@v1

    - name: Configure CMake
      if: steps.clang_check.outputs.skip_build != 'true'
      run: |
        cmake -B ${{ github.workspace }}/build -G "${{ matrix.config.generator }}" -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} -DCMAKE_C_COMPILER=${{ matrix.config.cc }} -DCMAKE_CXX_COMPILER=${{ matrix.config.cxx }} -DOULY_BUILD_TESTS=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
      env:
        CC: ${{ matrix.config.cc }}
        CXX: ${{ matrix.config.cxx }}

    - name: Build
      if: steps.clang_check.outputs.skip_build != 'true'
      run: cmake --build ${{ github.workspace }}/build --config ${{ env.BUILD_TYPE }} --parallel

    - name: Test
      if: steps.clang_check.outputs.skip_build != 'true'
      working-directory: ${{ github.workspace }}/build
      run: ctest -C ${{ env.BUILD_TYPE }} --output-on-failure --parallel

    - name: Run Performance Benchmarks
      if: steps.clang_check.outputs.skip_build != 'true' && github.event_name == 'push' && contains(fromJSON('["main", "develop"]'), github.ref_name)
      working-directory: ${{ github.workspace }}/build/unit_tests
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
      run: |
        if [ -f "./bench_scheduler_comparison" ] || [ -f "./bench_scheduler_comparison.exe" ]; then
          echo "ðŸš€ Running performance benchmarks..."
          if [ -f "./bench_scheduler_comparison" ]; then
            ./bench_scheduler_comparison --quick
          else
            ./bench_scheduler_comparison.exe --quick
          fi
          echo "âœ… Benchmarks completed"
          
          # List generated files
          echo "Generated benchmark files:"
          ls -la *.json *.txt 2>/dev/null || echo "No benchmark files found"
        else
          echo "âš ï¸ Benchmark executable not found, skipping benchmarks"
        fi

    - name: Upload benchmark results
      if: steps.clang_check.outputs.skip_build != 'true' && github.event_name == 'push' && contains(fromJSON('["main", "develop"]'), github.ref_name)
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.config.name }}-${{ github.run_number }}
        path: |
          ${{ github.workspace }}/build/unit_tests/*.json
          ${{ github.workspace }}/build/unit_tests/*.txt
        retention-days: 30
        if-no-files-found: warn

  coverage:
    name: Code Coverage
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup CMake and Ninja
      uses: lukka/get-cmake@latest

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y gcc-14 g++-14 lcov

    - name: Configure CMake with coverage
      run: |
        cmake -B ${{ github.workspace }}/build -G "Ninja" -DCMAKE_BUILD_TYPE=Debug -DCMAKE_C_COMPILER=gcc-14 -DCMAKE_CXX_COMPILER=g++-14 -DOULY_BUILD_TESTS=ON -DOULY_TEST_COVERAGE=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
      env:
        CC: gcc-14
        CXX: g++-14

    - name: Build with coverage
      run: cmake --build ${{ github.workspace }}/build --config Debug --parallel

    - name: Run tests
      working-directory: ${{ github.workspace }}/build
      run: ctest -C Debug --output-on-failure --parallel

    - name: Generate coverage report
      working-directory: ${{ github.workspace }}/build
      run: |
        lcov --gcov-tool gcov-14 --capture --directory . --base-directory ${{ github.workspace }} --output-file coverage.info --no-external --ignore-errors mismatch
        lcov --remove coverage.info '/usr/*' '*/_deps/*' '*/catch2-src/*' --output-file coverage.info --ignore-errors unused

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ${{ github.workspace }}/build/coverage.info
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: false

  documentation:
    name: Documentation
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Doxygen
      run: |
        sudo apt-get update
        sudo apt-get install -y doxygen graphviz

    - name: Install Python dependencies
      working-directory: ${{ github.workspace }}/docs
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Build documentation
      working-directory: ${{ github.workspace }}/docs
      run: make html

    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs/_build/html

  perfo:
    name: Performance Tracking
    runs-on: ubuntu-latest
    needs: build-and-test
    if: github.event_name == 'push' && contains(fromJSON('["main", "develop"]'), github.ref_name)

    steps:
    - name: Checkout main repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Download all benchmark results
      uses: actions/download-artifact@v4
      with:
        path: benchmark-artifacts
        pattern: benchmark-results-*
        merge-multiple: true

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install matplotlib pandas numpy

    - name: Checkout performance tracking branch
      run: |
        git fetch origin perfo || git checkout --orphan perfo
        git checkout perfo || echo "Creating new perfo branch"

    - name: Setup performance tracking branch structure
      run: |
        # Create directory structure if it doesn't exist
        mkdir -p results
        mkdir -p scripts
        
        # Copy scripts to perfo branch
        if [ -d "../scripts" ]; then
          cp -r ../scripts/* scripts/ 2>/dev/null || echo "No scripts to copy"
        fi
        
        # Make scripts executable
        chmod +x scripts/*.py 2>/dev/null || echo "No Python scripts to make executable"

    - name: Process and organize benchmark results
      run: |
        echo "ðŸ”„ Processing benchmark results..."
        
        # Get metadata
        COMMIT_HASH="${{ github.sha }}"
        COMMIT_SHORT="${COMMIT_HASH:0:8}"
        BUILD_NUMBER="${{ github.run_number }}"
        
        # Count collected files
        COLLECTED_FILES=0
        if [ -d "../benchmark-artifacts" ]; then
          COLLECTED_FILES=$(find ../benchmark-artifacts -name "*.json" -o -name "*.txt" | wc -l)
          echo "Found $COLLECTED_FILES benchmark files"
          
          if [ $COLLECTED_FILES -gt 0 ]; then
            # Copy files to results directory
            cp ../benchmark-artifacts/*.json results/ 2>/dev/null || echo "No JSON files to copy"
            cp ../benchmark-artifacts/*.txt results/ 2>/dev/null || echo "No TXT files to copy"
            
            echo "âœ… Copied benchmark files to results directory"
            ls -la results/
          else
            echo "âš ï¸ No benchmark files found in artifacts"
          fi
        else
          echo "âš ï¸ No benchmark artifacts directory found"
        fi

    - name: Cleanup old results
      run: |
        if [ -f "scripts/cleanup_old_results.py" ]; then
          echo "ðŸ§¹ Cleaning up old results (keeping last 20 builds)..."
          python3 scripts/cleanup_old_results.py results --keep 20
        else
          echo "âš ï¸ Cleanup script not found, skipping cleanup"
        fi

    - name: Generate performance visualizations and report
      run: |
        if [ -f "scripts/visualize_performance.py" ]; then
          echo "ðŸ“Š Generating performance visualizations..."
          python3 scripts/visualize_performance.py results -o . -v
          
          if [ -f "PERFORMANCE.md" ]; then
            echo "âœ… PERFORMANCE.md generated successfully"
          else
            echo "âš ï¸ PERFORMANCE.md was not generated"
          fi
        else
          echo "âš ï¸ Visualization script not found, creating basic report"
          echo "# OULY Performance Report" > PERFORMANCE.md
          echo "" >> PERFORMANCE.md
          echo "**Generated:** $(date)" >> PERFORMANCE.md
          echo "**Commit:** ${{ github.sha }}" >> PERFORMANCE.md
          echo "**Build:** ${{ github.run_number }}" >> PERFORMANCE.md
          echo "" >> PERFORMANCE.md
          echo "Benchmark results are available in the results directory." >> PERFORMANCE.md
        fi

    - name: Commit results to perfo branch
      run: |
        # Configure git
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Add all files
        git add .
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          # Count files in results directory
          RESULT_COUNT=$(find results -name "*.json" -o -name "*.txt" | wc -l)
          
          git commit -m "ðŸ“Š Performance results for build ${{ github.run_number }}
          
          Commit: ${{ github.sha }}
          Branch: ${{ github.ref_name }}
          Total result files: $RESULT_COUNT
          Generated: $(date)"
          
          git push origin perfo
          echo "âœ… Results committed and pushed to perfo branch"
        fi
