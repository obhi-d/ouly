name: Performance Benchmarks

on:
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - scheduler
        - allocator
  schedule:
    # Run performance benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  push:
    branches: [ main ]
  workflow_run:
    workflows: ["Continuous Integration"]
    branches: [ main ]
    types: 
      - completed

env:
  BUILD_TYPE: Release
  CMAKE_BUILD_PARALLEL_LEVEL: 4

jobs:
  dedicated-benchmarks:
    name: Performance Testing (${{ matrix.os }}-${{ matrix.compiler }})
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        compiler: 
          - name: gcc-14
            cc: gcc-14
            cxx: g++-14
            id: gcc-14
          - name: clang-18
            cc: clang-18
            cxx: clang++-18
            id: clang-18
              
    runs-on: ${{ matrix.os }}
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up compiler (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        if [[ "${{ matrix.compiler.name }}" == "gcc-14" ]]; then
          sudo apt-get install -y gcc-14 g++-14
        elif [[ "${{ matrix.compiler.name }}" == "clang-18" ]]; then
          wget https://apt.llvm.org/llvm.sh
          chmod +x llvm.sh
          sudo ./llvm.sh 18
          sudo apt-get install -y clang-18 clang++-18
        fi
        echo "CC=${{ matrix.compiler.cc }}" >> $GITHUB_ENV
        echo "CXX=${{ matrix.compiler.cxx }}" >> $GITHUB_ENV
        echo "COMPILER_ID=${{ matrix.compiler.id }}" >> $GITHUB_ENV
    
    - name: Configure CMake (Release build without ASAN)
      run: |
        cmake -B build \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_C_COMPILER=$CC \
          -DCMAKE_CXX_COMPILER=$CXX \
          -DOULY_BUILD_TESTS=ON \
          -DOULY_ASAN_ENABLED=OFF
    
    - name: Build
      run: cmake --build build --config ${{ env.BUILD_TYPE }} --parallel
    
    - name: Run Extended Performance Benchmarks
      working-directory: build/unit_tests
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
        COMPILER_ID: ${{ env.COMPILER_ID }}
      run: |
        echo "🚀 Running extended performance benchmarks..."
        echo "Environment: COMPILER_ID=$COMPILER_ID, BUILD_NUMBER=$GITHUB_RUN_NUMBER"
        echo "Commit SHA: $GITHUB_SHA"
        
        # Default benchmark type for scheduled/push events
        BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'all' }}"
        
        # Run comprehensive scheduler comparison benchmarks
        if [[ "$BENCHMARK_TYPE" == "all" || "$BENCHMARK_TYPE" == "scheduler" ]]; then
          echo "📊 Running comprehensive scheduler benchmarks..."
          if [ -f "./bench_scheduler_comparison" ]; then
            ./bench_scheduler_comparison
          else
            echo "⚠️ Scheduler benchmark executable not found"
          fi
        fi
        
        # Run general performance benchmarks with proper naming
        if [[ "$BENCHMARK_TYPE" == "all" || "$BENCHMARK_TYPE" == "allocator" ]]; then
          echo "🔄 Running general performance benchmarks..."
          if [ -f "./bench_performance" ]; then
            # Generate output filename following the expected pattern
            OUTPUT_FILE="${COMPILER_ID}-$(echo $GITHUB_SHA | cut -c1-8)-$GITHUB_RUN_NUMBER-allocator_performance.json"
            echo "Saving allocator benchmark results to: $OUTPUT_FILE"
            ./bench_performance "$OUTPUT_FILE"
          else
            echo "⚠️ General performance benchmark executable not found"
          fi
        fi
        
        echo "✅ Extended benchmarks completed"
        echo "Generated files:"
        ls -la *.json *.txt 2>/dev/null || echo "No result files found"
        
    - name: Upload extended benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: extended-benchmark-results-${{ matrix.os }}-${{ matrix.compiler.id }}-${{ github.run_number }}
        path: |
          build/unit_tests/*.json
          build/unit_tests/*.txt
        retention-days: 90

  update-perfo:
    name: Update Performance Tracking
    runs-on: ubuntu-latest
    needs: dedicated-benchmarks
    if: always() && (github.ref == 'refs/heads/main' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success'))
    
    steps:
    - name: Checkout main repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install matplotlib pandas numpy

    - name: Checkout performance tracking branch
      run: |
        git fetch origin perfo || echo "No existing perfo branch"

        # If the branch already exists on the remote, make the local branch
        # point **exactly** at it so we start from an up‑to‑date tip.
        if git show-ref --quiet refs/remotes/origin/perfo; then
          git checkout -B perfo origin/perfo
        else
          # First‑time run: create a clean orphan branch so history is separate
          git checkout --orphan perfo
        fi

    - name: Download all extended benchmark results
      uses: actions/download-artifact@v4
      with:
        path: new_results
        pattern: extended-benchmark-results-*
        merge-multiple: true

    - name: Process extended benchmark results
      run: |
        echo "🔄 Processing extended benchmark results..."
        
        # Ensure directory structure exists
        mkdir -p results scripts
        
        # Merge new results with existing ones (preserving history)
        if [ -d "new_results" ] && [ "$(find new_results -name '*.json' -o -name '*.txt' | wc -l)" -gt 0 ]; then
          echo "📦 Merging new results with existing historical data..."
          echo "Existing results: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
          echo "New results: $(find new_results -name '*.json' -o -name '*.txt' | wc -l) files"
          
          # Copy new results to results directory
          cp -r new_results/* results/ 2>/dev/null || echo "No new result files to copy"
          
          echo "Total results after merge: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
        else
          echo "⚠️ No new results found to merge"
        fi
        
        # Copy scripts from main branch
        git checkout main -- scripts/ || echo "No scripts directory in main branch"
        chmod +x scripts/*.py 2>/dev/null || echo "No Python scripts to make executable"
        
    - name: Cleanup old results
      run: |
        if [ -f "scripts/cleanup_old_results.py" ]; then
          echo "🧹 Cleaning up old results (keeping last 20 builds)..."
          echo "Results before cleanup: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
          python3 scripts/cleanup_old_results.py results --keep 20
          echo "Results after cleanup: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
        else
          echo "⚠️ Cleanup script not found, keeping all results"
        fi

    - name: Generate comprehensive performance report
      run: |
        if [ -f "scripts/visualize_performance.py" ]; then
          echo "📊 Generating comprehensive performance visualizations..."
          echo "Available result files:"
          find results -name "*.json" | head -10
          python3 scripts/visualize_performance.py results -o . -v
          
          echo "Generated visualization files:"
          ls -la *.svg 2>/dev/null || echo "No SVG files generated"
          
          echo "Generated report:"
          ls -la PERFORMANCE.md 2>/dev/null || echo "No PERFORMANCE.md generated"
        else
          echo "⚠️ Visualization script not found"
          echo "# OULY Performance Report (Extended)" > PERFORMANCE.md
          echo "**Generated:** $(date)" >> PERFORMANCE.md
          echo "**Type:** Extended/Dedicated Benchmarks" >> PERFORMANCE.md
          echo "**Build Number:** ${{ github.run_number }}" >> PERFORMANCE.md
          echo "**Commit:** ${{ github.sha }}" >> PERFORMANCE.md
          echo "" >> PERFORMANCE.md
          echo "⚠️ Performance visualization script was not available during this run." >> PERFORMANCE.md
        fi

    - name: Commit extended results
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Remove the temporary new_results directory
        rm -rf new_results
        
        # Stage all the files we want to preserve on perfo branch
        git add PERFORMANCE.md || true
        git add results/ || true
        git add *.svg 2>/dev/null || true
        git add scripts/ || true

        if ! git diff --staged --quiet; then
          RESULT_COUNT=$(find results -name "*.json" -o -name "*.txt" 2>/dev/null | wc -l)
          git commit -m "📊 Extended performance results (build ${{ github.run_number }})

          Commit: ${{ github.sha }}
          Type: Extended/Dedicated benchmarks
          Total result files: $RESULT_COUNT
          Generated: $(date)"
          
          # Rebase onto the latest remote tip to handle concurrent updates
          git pull --rebase --autostash origin perfo || true

          # Push with force-with-lease to prevent overwriting concurrent changes
          git push --force-with-lease origin perfo
        else
          echo "No new extended results to commit"
        fi
        