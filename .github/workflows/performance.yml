name: Performance Benchmarks

on:
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - scheduler
        - allocator
  push:
    branches: [ main ]
  workflow_run:
    workflows: ["Continuous Integration"]
    branches: [ main ]
    types: 
      - completed

env:
  BUILD_TYPE: Release
  CMAKE_BUILD_PARALLEL_LEVEL: 4

jobs:
  dedicated-benchmarks:
    name: Performance Testing (${{ matrix.os }}-${{ matrix.compiler }})
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        compiler: 
          - name: gcc-14
            cc: gcc-14
            cxx: g++-14
            id: gcc-14
          - name: clang-18
            cc: clang-18
            cxx: clang++-18
            id: clang-18
              
    runs-on: ${{ matrix.os }}
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up compiler (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        if [[ "${{ matrix.compiler.name }}" == "gcc-14" ]]; then
          sudo apt-get install -y gcc-14 g++-14
        elif [[ "${{ matrix.compiler.name }}" == "clang-18" ]]; then
          wget https://apt.llvm.org/llvm.sh
          chmod +x llvm.sh
          sudo ./llvm.sh 18
          sudo apt-get install -y clang-18 clang++-18
        fi
        echo "CC=${{ matrix.compiler.cc }}" >> $GITHUB_ENV
        echo "CXX=${{ matrix.compiler.cxx }}" >> $GITHUB_ENV
        echo "COMPILER_ID=${{ matrix.compiler.id }}" >> $GITHUB_ENV
    
    - name: Configure CMake (Release build without ASAN)
      run: |
        cmake -B build \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_C_COMPILER=$CC \
          -DCMAKE_CXX_COMPILER=$CXX \
          -DOULY_BUILD_TESTS=ON \
          -DOULY_ASAN_ENABLED=OFF
    
    - name: Build
      run: cmake --build build --config ${{ env.BUILD_TYPE }} --parallel
    
    - name: Run Extended Performance Benchmarks
      working-directory: build/unit_tests
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
        COMPILER_ID: ${{ env.COMPILER_ID }}
      run: |
        echo "ðŸš€ Running extended performance benchmarks..."
        echo "Environment: COMPILER_ID=$COMPILER_ID, BUILD_NUMBER=$GITHUB_RUN_NUMBER"
        echo "Commit SHA: $GITHUB_SHA"
        
        # Default benchmark type for scheduled/push events
        BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'all' }}"
        
        # Run comprehensive scheduler comparison benchmarks
        if [[ "$BENCHMARK_TYPE" == "all" || "$BENCHMARK_TYPE" == "scheduler" ]]; then
          echo "ðŸ“Š Running comprehensive scheduler benchmarks..."
          if [ -f "./bench_scheduler_comparison" ]; then
            ./bench_scheduler_comparison
          else
            echo "âš ï¸ Scheduler benchmark executable not found"
          fi
        fi
        
        # Run general performance benchmarks with proper naming
        if [[ "$BENCHMARK_TYPE" == "all" || "$BENCHMARK_TYPE" == "allocator" ]]; then
          echo "ðŸ”„ Running general performance benchmarks..."
          if [ -f "./bench_performance" ]; then
            # Generate output filename following the expected pattern
            OUTPUT_FILE="${COMPILER_ID}-$(echo $GITHUB_SHA | cut -c1-8)-$GITHUB_RUN_NUMBER-allocator_performance.json"
            echo "Saving allocator benchmark results to: $OUTPUT_FILE"
            ./bench_performance "$OUTPUT_FILE"
          else
            echo "âš ï¸ General performance benchmark executable not found"
          fi
        fi
        
        echo "âœ… Extended benchmarks completed"
        echo "Generated files:"
        ls -la *.json *.txt 2>/dev/null || echo "No result files found"
        
    - name: Upload extended benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: extended-benchmark-results-${{ matrix.os }}-${{ matrix.compiler.id }}-${{ github.run_number }}
        path: |
          build/unit_tests/*.json
          build/unit_tests/*.txt
        retention-days: 90

  update-perfo:
    name: Update Performance Tracking
    runs-on: ubuntu-latest
    needs: dedicated-benchmarks
    if: always() && (github.ref == 'refs/heads/main' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success'))
    
    steps:
    - name: Checkout main repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install matplotlib pandas numpy

    - name: Checkout performance tracking branch
      run: |
        git fetch origin perfo || echo "No existing perfo branch"

        # If the branch already exists on the remote, make the local branch
        # point **exactly** at it so we start from an upâ€‘toâ€‘date tip.
        if git show-ref --quiet refs/remotes/origin/perfo; then
          git checkout -B perfo origin/perfo
        else
          # Firstâ€‘time run: create a clean orphan branch so history is separate
          git checkout --orphan perfo
        fi

    - name: Download all extended benchmark results
      uses: actions/download-artifact@v4
      with:
        path: last_results
        pattern: extended-benchmark-results-*
        merge-multiple: true

    - name: Copy results from last run to previous runs directory
      run: |
        mkdir -p results
        if [ -d "last_results" ]; then
          echo "ðŸ“¦ Copying results from last run..."
          cp -r last_results/* results/ || echo "No results found in last run"
        else
          echo "âš ï¸ No last_results directory found"
        fi
   
    - name: Cleanup old results
      run: |
        if [ -f "scripts/cleanup_old_results.py" ]; then
          echo "ðŸ§¹ Cleaning up old results (keeping last 30 builds)..."
          echo "Results before cleanup: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
          python3 scripts/cleanup_old_results.py results --keep 30
          echo "Results after cleanup: $(find results -name '*.json' -o -name '*.txt' 2>/dev/null | wc -l) files"
        else
          echo "âš ï¸ Cleanup script not found, keeping all results"
        fi
        
    - name: Generate comprehensive performance report
      run: |
        if [ -f "scripts/visualize_performance.py" ]; then
          echo "ðŸ“Š Generating comprehensive performance visualizations..."
          echo "Available result files:"
          find results -name "*.json" | head -10
          python3 scripts/visualize_performance.py results -o . -v
          
          echo "Generated visualization files:"
          ls -la *.svg 2>/dev/null || echo "No SVG files generated"
          
          echo "Generated report:"
          ls -la PERFORMANCE.md 2>/dev/null || echo "No PERFORMANCE.md generated"
        else
          echo "âš ï¸ Visualization script not found"
          echo "# OULY Performance Report (Extended)" > PERFORMANCE.md
          echo "**Generated:** $(date)" >> PERFORMANCE.md
          echo "**Type:** Extended/Dedicated Benchmarks" >> PERFORMANCE.md
          echo "**Build Number:** ${{ github.run_number }}" >> PERFORMANCE.md
          echo "**Commit:** ${{ github.sha }}" >> PERFORMANCE.md
          echo "" >> PERFORMANCE.md
          echo "âš ï¸ Performance visualization script was not available during this run." >> PERFORMANCE.md
        fi

    - name: Commit extended results
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Remove the temporary new_results directory
        rm -rf last_results || echo "No last_results directory to remove"
        
        # Stage all the files we want to preserve on perfo branch
        git add PERFORMANCE.md || true
        git add *.svg 2>/dev/null || true
        git add results/*.json 2>/dev/null || true
        git add results/*.txt 2>/dev/null || true

        if ! git diff --staged --quiet; then
          git commit -m "ðŸ“Š Extended performance results (build ${{ github.run_number }})

          Commit: ${{ github.sha }}
          Type: Extended/Dedicated benchmarks
          Total result files: $RESULT_COUNT
          Generated: $(date)"
          
          # Rebase onto the latest remote tip to handle concurrent updates
          git pull --rebase --autostash origin perfo || true

          # Push with force-with-lease to prevent overwriting concurrent changes
          git push --force-with-lease origin perfo
        else
          echo "No new extended results to commit"
        fi
